{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkidia/Crop-Type-classfication_Senengal_DL/blob/main/S2_Data_Extraction_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nourGimdHTd9"
      },
      "source": [
        "# Reading corner\n",
        "SEN4STAT/ESA: https://www.esa-sen4stat.org/user-stories/senegal-prototype/\n",
        "\n",
        "EOSTAT/FAO: https://data.apps.fao.org/catalog/dataset/5c377b2b-3c2e-4b70-afd7-0c80900b68bb/resource/50bc9ff5-95d2-40cd-af12-6aee2cfcc4ae\n",
        "\n",
        "RNN: https://www.sciencedirect.com/science/article/pii/S0034425721003230#bb0310 and its GitHub : https://github.com/0zgur0/multi-stage-convSTAR-network/tree/master\n",
        "\n",
        "https://www.sciencedirect.com/science/article/pii/S0034425724001214\n",
        "\n",
        "https://www.sciencedirect.com/science/article/abs/pii/S2352938522001203\n",
        "\n",
        "crop type mapping in Ghana and South Sudan: GitHubCode: https://github.com/roserustowicz/crop-type-mapping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MuSmeLZPHYE6"
      },
      "outputs": [],
      "source": [
        "#Authotication\n",
        "import ee\n",
        "# @title Authenticate to the Earth Engine servers\n",
        "ee.Authenticate()\n",
        "# Initialize the Earth Engine object with Google Cloud project ID\n",
        "project_id = 'ee-kkidia3' # change here\n",
        "ee.Initialize(project=project_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rasterio"
      ],
      "metadata": {
        "id": "gNP5KrLzYG37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wGpJhpXiH2V-"
      },
      "outputs": [],
      "source": [
        "# @title Lib imports:\n",
        "#import ee\n",
        "#print('Using EE version ', ee.__version__)\n",
        "import folium\n",
        "from folium.plugins import DualMap\n",
        "from branca.element import Template, MacroElement\n",
        "#print('Using Folium version ', folium.__version__)\n",
        "from os import MFD_HUGE_1MB\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, Iterable, List, Tuple\n",
        "#from google.colab import auth\n",
        "import datetime as dt\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import shape, Polygon, MultiPolygon\n",
        "from shapely import wkt\n",
        "\n",
        "from google.cloud import storage\n",
        "import tensorflow as tf\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N4HKjHajIBtj"
      },
      "outputs": [],
      "source": [
        "#@title Clean data 18, 19, 20, and 2023\n",
        "def load_cleandata(asset_id):\n",
        "    return ee.FeatureCollection(asset_id)\n",
        "clean2018 = load_cleandata('projects/ee-kkidia3/assets/clean_data_2018-2023_no-bands/clean_raw_data_2018')\n",
        "clean2019 = load_cleandata('projects/ee-kkidia3/assets/clean_data_2018-2023_no-bands/clean_raw_data_2019')\n",
        "clean2020 = load_cleandata('projects/ee-kkidia3/assets/clean_data_2018-2023_no-bands/clean_raw_data_2020')\n",
        "clean2023 = load_cleandata('projects/ee-kkidia3/assets/clean_data_2018-2023_no-bands/clean_raw_data_2023')\n",
        "\n",
        "asset_clean = {\n",
        "    'clean2018': 'projects/ee-kkidia3/assets/clean_data_2018-2023_no-bands/clean_raw_data_2018',  # clean Data 2018\n",
        "    'clean2019': 'projects/ee-kkidia3/assets/clean_data_2018-2023_no-bands/clean_raw_data_2019',  # clean Data 2019\n",
        "    'clean2020': 'projects/ee-kkidia3/assets/clean_data_2018-2023_no-bands/clean_raw_data_2020',  # clean Data 2020\n",
        "    'clean2023': 'projects/ee-kkidia3/assets/clean_data_2018-2023_no-bands/clean_raw_data_2023',  #clean Data 2023\n",
        "}\n",
        "clean_data = clean2018.merge(clean2019).merge(clean2020).merge(clean2023)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIUu9PzwIGXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b217d1b-8f86-4961-d7fb-12cfd6400743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17836\n",
            "1433\n"
          ]
        }
      ],
      "source": [
        "print(clean_data.size().getInfo())\n",
        "print(clean2019.size().getInfo())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Read polygons properties\n",
        "\n",
        "# Define a function to fetch features in batches\n",
        "def get_feature_batch(collection, batch_size=5000):\n",
        "    # Initialize an empty list to store all features\n",
        "    all_features = []\n",
        "\n",
        "    # Get the total number of features in the collection\n",
        "    total_count = collection.size().getInfo()\n",
        "\n",
        "    # Loop through the collection in batches\n",
        "    for i in range(0, total_count, batch_size):\n",
        "        # Fetch the current batch of features\n",
        "        batch = collection.toList(batch_size, i).map(lambda f: ee.Feature(f).toDictionary()).getInfo()\n",
        "\n",
        "        # Add the batch to the list of all features\n",
        "        all_features.extend(batch)\n",
        "\n",
        "    return all_features\n",
        "\n",
        "# Use the function to get all features from the collection\n",
        "feature_properties = get_feature_batch(ee.FeatureCollection(clean_data))\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(feature_properties)\n",
        "\n",
        "# Show the first few rows\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "vr7dS1QiKW2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from google.colab import sheets\n",
        "sheet = sheets.InteractiveSheet(df=df)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "id": "hUVHu5zjCsiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Labels\n",
        "import pandas as pd\n",
        "\n",
        "# Function to fetch features in batches\n",
        "def get_feature_batch(collection, batch_size=5000):\n",
        "    # Initialize an empty list to store all features\n",
        "    all_features = []\n",
        "\n",
        "    # Get the total number of features in the collection\n",
        "    total_count = collection.size().getInfo()\n",
        "\n",
        "    # Loop through the collection in batches\n",
        "    for i in range(0, total_count, batch_size):\n",
        "        # Fetch the current batch of features\n",
        "        batch = collection.toList(batch_size, i).map(lambda f: ee.Feature(f).toDictionary()).getInfo()\n",
        "\n",
        "        # Add the batch to the list of all features\n",
        "        all_features.extend(batch)\n",
        "\n",
        "    return all_features\n",
        "\n",
        "# Use the function to get all features from the collection\n",
        "feature_properties = get_feature_batch(ee.FeatureCollection(clean_data))\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(feature_properties)\n",
        "\n",
        "# Group the DataFrame by \"Name\", and count occurrences, keeping \"Class\" and \"Sub-class\"\n",
        "grouped_df = df.groupby(['Name', 'Class', 'Sub_class']).size().reset_index(name='Count')\n",
        "\n",
        "# Export the grouped data to a CSV file\n",
        "grouped_df.to_csv('grouped_polygons.csv', index=False)\n",
        "\n",
        "# To display the resulting CSV file using Google Colab sheets\n",
        "from google.colab import sheets\n",
        "sheet = sheets.InteractiveSheet(df=grouped_df)\n",
        "\n",
        "# Show the first few rows\n",
        "grouped_df.head()\n"
      ],
      "metadata": {
        "id": "wiXiQCklQYhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract Sentinel temporal data\n",
        "\n",
        "# Load polygon asset.\n",
        "polygons = ee.FeatureCollection('projects/ee-kkidia3/assets/crop2023/okra')  # simple dataset for trial and error\n",
        "\n",
        "# Define the date range for August, September, and October of 2019.\n",
        "start_date = '2019-08-01'\n",
        "end_date = '2019-10-31'\n",
        "\n",
        "# Load the Sentinel-2 Harmonized ImageCollection.\n",
        "s2_collection = ee.ImageCollection('COPERNICUS/S2_HARMONIZED') \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .filterBounds(polygons) \\\n",
        "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \\\n",
        "    .select(['B2', 'B3', 'B4', 'B8', 'QA60'])  # Blue, Green, Red, NIR bands at 10m resolution + QA60 for cloud masking.\n",
        "\n",
        "# Function to mask clouds and cirrus using the QA60 band\n",
        "def mask_s2_clouds(image):\n",
        "    qa = image.select('QA60')  # Select the QA60 band\n",
        "\n",
        "    # Bits 10 and 11 are for clouds and cirrus respectively\n",
        "    cloud_bit_mask = 1 << 10\n",
        "    cirrus_bit_mask = 1 << 11\n",
        "\n",
        "    # Create a mask for clear conditions\n",
        "    mask = qa.bitwiseAnd(cloud_bit_mask).eq(0).And(qa.bitwiseAnd(cirrus_bit_mask).eq(0))\n",
        "\n",
        "    # Return the masked image\n",
        "    return image.updateMask(mask).copyProperties(image, [\"system:time_start\"])\n",
        "\n",
        "# Function to normalize the pixel values of each band (min-max scaling).\n",
        "def normalize_bands(image):\n",
        "    band_min_max = {\n",
        "        'B2': (0, 10000),  # Min and max values for Blue band\n",
        "        'B3': (0, 10000),  # Min and max values for Green band\n",
        "        'B4': (0, 10000),  # Min and max values for Red band\n",
        "        'B8': (0, 10000),  # Min and max values for NIR band\n",
        "    }\n",
        "\n",
        "    # Normalize each band (standard min-max scaling).\n",
        "    normalized = ee.Image([\n",
        "        image.select(band).subtract(band_min_max[band][0]).divide(band_min_max[band][1] - band_min_max[band][0])\n",
        "        for band in ['B2', 'B3', 'B4', 'B8']\n",
        "    ]).rename(['B2', 'B3', 'B4', 'B8'])\n",
        "\n",
        "    return normalized\n",
        "\n",
        "# Apply cloud masking and normalization to the image collection\n",
        "processed_collection = s2_collection.map(mask_s2_clouds).map(normalize_bands)\n",
        "\n"
      ],
      "metadata": {
        "id": "3GGgN9yWqRI0"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title count number of imegeries per polygon\n",
        "\n",
        "# Function to check which polygons have imagery and count the number of images\n",
        "def check_imagery_count(feature):\n",
        "    # Get the patch geometry based on the feature (polygon)\n",
        "    centroid = feature.geometry().centroid()\n",
        "    buffer_distance = 320  # Buffer distance to match 640m x 640m patch\n",
        "    patch_geometry = centroid.buffer(buffer_distance).bounds()\n",
        "\n",
        "    # Filter images that intersect the patch geometry\n",
        "    images = processed_collection.filterBounds(patch_geometry)\n",
        "\n",
        "    # Get the number of images for this polygon\n",
        "    image_count = images.size()\n",
        "\n",
        "    # Return the feature with the image count as a new property\n",
        "    # Removed the client-side operations (print and getInfo) from within the mapped function\n",
        "    return feature.set({'image_count': image_count})\n",
        "\n",
        "# Map the function over all polygons to check imagery availability\n",
        "polygons_with_imagery = polygons.map(check_imagery_count)\n",
        "\n",
        "# Print out the total number of images for each polygon\n",
        "polygon_info = polygons_with_imagery.aggregate_array('image_count').getInfo()\n",
        "total_polygons = len(polygon_info)\n",
        "polygons_with_images = sum(1 for count in polygon_info if count > 0)\n",
        "\n",
        "print(f\"Total polygons: {total_polygons}\")\n",
        "print(f\"Polygons with imagery: {polygons_with_images}\")\n",
        "print(f\"Polygons without imagery: {total_polygons - polygons_with_images}\")\n",
        "\n",
        "# Iterate over the features in the collection and print the information\n",
        "for i in range(total_polygons):\n",
        "  if polygon_info[i] > 0:\n",
        "    print(f\"Polygon ID: {i} has {polygon_info[i]} images.\")\n",
        "  else:\n",
        "    print(f\"Polygon ID: {i} has no images.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0aItBOn4fwI",
        "outputId": "f0525761-2220-4293-b1b7-10bfc816e79a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total polygons: 9\n",
            "Polygons with imagery: 9\n",
            "Polygons without imagery: 0\n",
            "Polygon ID: 0 has 7 images.\n",
            "Polygon ID: 1 has 7 images.\n",
            "Polygon ID: 2 has 3 images.\n",
            "Polygon ID: 3 has 4 images.\n",
            "Polygon ID: 4 has 5 images.\n",
            "Polygon ID: 5 has 5 images.\n",
            "Polygon ID: 6 has 5 images.\n",
            "Polygon ID: 7 has 5 images.\n",
            "Polygon ID: 8 has 7 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGwRPdahpMle"
      },
      "source": [
        "Export Sentinel imegeries to Pytorch data environment\n",
        "\n",
        "here is the steps:\n",
        "1. Include Polygon Labels: Add polygon labels as metadata in the exported TFRecord.\n",
        "2. Spatial Metadata: Include Coordinate Reference Systems (CRS) and Affine transformations in the GEOTIF.\n",
        "3. Normalization of Bands: Normalize the pixel values of the spectral bands.\n",
        "4. Batching and Prefetching: Adjust the export logic to avoid overlaps and improve performance by batching and prefetching."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Export as GeoTIFF\n",
        "# Function to create a fixed 64x64 pixel patch with consistent size and projection\n",
        "def create_fixed_size_patch(feature):\n",
        "    # Get the centroid of the polygon\n",
        "    centroid = feature.geometry().centroid()\n",
        "\n",
        "    # Define the buffer distance as 320 meters to create a 640m x 640m patch\n",
        "    buffer_distance = 320  # Half of the patch size in meters\n",
        "    patch_geometry = centroid.buffer(buffer_distance).bounds()\n",
        "\n",
        "    # Define a consistent UTM projection, adjust UTM zone based on the region (e.g., EPSG:32630 for West Africa)\n",
        "    utm_projection = ee.Projection('EPSG:32630').atScale(10)  # 10-meter resolution\n",
        "\n",
        "    # Filter images that intersect the patch geometry\n",
        "    images = s2_collection.filterBounds(patch_geometry)\n",
        "\n",
        "    # Apply cloud masking and normalization\n",
        "    images = images.map(mask_s2_clouds).map(normalize_bands)\n",
        "\n",
        "    def clip_and_reproject_image(image):\n",
        "        # Clip the image to the fixed-size geometry (640m x 640m)\n",
        "        clipped_image = image.clip(patch_geometry)\n",
        "\n",
        "        # Reproject to the defined UTM projection with a 10m resolution\n",
        "        reprojected_image = clipped_image.reproject(utm_projection)\n",
        "\n",
        "        # Get the 'system:time_start' property\n",
        "        date = image.get('system:time_start')\n",
        "\n",
        "        # Format the date if available, otherwise use an empty string for the date part\n",
        "        formatted_date = ee.Algorithms.If(date, ee.Date(date).format('YYYY-MM-dd'), '')\n",
        "\n",
        "        # Set metadata to ensure each image has consistent affine transformation and CRS\n",
        "        reprojected_image = reprojected_image.set({\n",
        "            'polygon_id': feature.id(),\n",
        "            'label': feature.get('label'),\n",
        "            'date': formatted_date,  # Use the exact imagery date if available, else ''\n",
        "            'crs': utm_projection.crs(),\n",
        "            'affine': utm_projection.transform(),\n",
        "            'system_index': image.get('system:index')\n",
        "        })\n",
        "\n",
        "        return reprojected_image\n",
        "\n",
        "    # Apply the clipping and reproject function to each image\n",
        "    patches = images.map(clip_and_reproject_image)\n",
        "    return patches\n",
        "\n",
        "# Map the create_fixed_size_patch function over all polygons and flatten the result into an ImageCollection\n",
        "all_patches = polygons.map(create_fixed_size_patch).flatten()\n",
        "all_patches = ee.ImageCollection(all_patches)\n",
        "\n",
        "# Get the total number of patches\n",
        "patch_count = all_patches.size().getInfo()\n",
        "print(f'Total patches to export: {patch_count}')\n",
        "\n",
        "# Convert the collection to a list for iteration\n",
        "patches_list = all_patches.toList(patch_count)\n",
        "\n",
        "# Function to export each patch as GeoTIFF with explicit region size and projection\n",
        "def export_fixed_patch(index):\n",
        "    image = ee.Image(patches_list.get(index))\n",
        "    polygon_id = image.get('polygon_id').getInfo()\n",
        "    date = image.get('date').getInfo() or \"\"  # If no date, use an empty string\n",
        "    system_index = image.get('system_index').getInfo()\n",
        "\n",
        "    # Create the filename, include the date only if it's not empty\n",
        "    filename = f's2_patch_{polygon_id}{\"_\" + date if date else \"\"}_{system_index}'\n",
        "\n",
        "    # Define the export task with an explicit region (640m x 640m) and consistent UTM projection\n",
        "    task = ee.batch.Export.image.toCloudStorage(\n",
        "        image=image.toFloat(),\n",
        "        description=filename,\n",
        "        bucket='okra_foo',  # your bucket name\n",
        "        scale=10,  # 10-meter resolution\n",
        "        region=image.geometry().bounds().getInfo()['coordinates'],  # Explicitly set region\n",
        "        crs='EPSG:32630',  # Consistent UTM projection\n",
        "        fileFormat='GeoTIFF',\n",
        "        formatOptions={\n",
        "            'cloudOptimized': True  # Optimized for cloud storage\n",
        "        }\n",
        "    )\n",
        "    task.start()\n",
        "    print(f'Exporting {filename} as GeoTIFF')\n",
        "\n",
        "# Loop over the patches and export each one in batches\n",
        "batch_size = 10  # Adjust batch size as needed\n",
        "for i in range(0, patch_count, batch_size):\n",
        "    for j in range(i, min(i + batch_size, patch_count)):\n",
        "        export_fixed_patch(j)\n",
        "    print(f'Batch {i // batch_size + 1} exported.')\n"
      ],
      "metadata": {
        "id": "Deq_ptdlrayV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rasterio"
      ],
      "metadata": {
        "id": "cLGJVApb85MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import rasterio\n",
        "from rasterio.io import MemoryFile\n",
        "\n",
        "# Initialize Google Cloud Storage client\n",
        "client = storage.Client()\n",
        "\n",
        "# Define the bucket and file name\n",
        "bucket_name = 'okra_foo'  # Your GCS bucket\n",
        "file_name = 'planet_patch_00000000000000000007_20191027T112119_20191027T112939_T28PEV.tif'  # The GeoTIFF file in your GCS bucket\n",
        "\n",
        "# Access the file in the Google Cloud Storage bucket\n",
        "bucket = client.bucket(bucket_name)\n",
        "blob = bucket.blob(file_name)\n",
        "\n",
        "# Read the file into memory without downloading\n",
        "geo_tiff_data = blob.download_as_bytes()\n",
        "\n",
        "# Use rasterio to read the GeoTIFF from memory\n",
        "with MemoryFile(geo_tiff_data) as memfile:\n",
        "    with memfile.open() as dataset:\n",
        "        # Read the image data\n",
        "        image_data = dataset.read()  # Reads all bands into a numpy array\n",
        "\n",
        "        # Get metadata from the GeoTIFF\n",
        "        metadata = dataset.meta\n",
        "\n",
        "        # Print image shape and metadata\n",
        "        print(f\"Image shape: {image_data.shape}\")  # (bands, height, width)\n",
        "        print(\"Metadata:\", metadata)\n",
        "\n",
        "        # Print individual metadata items, e.g., CRS (Coordinate Reference System)\n",
        "        print(f\"CRS: {dataset.crs}\")\n",
        "        print(f\"Transform: {dataset.transform}\")\n",
        "\n",
        "        # You can also access other metadata fields\n",
        "        tags = dataset.tags()\n",
        "        print(\"Tags:\", tags)\n"
      ],
      "metadata": {
        "id": "vZrSrBRENl-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RGB\n",
        "import gcsfs\n",
        "import rasterio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Google Cloud Storage file system\n",
        "gcs = gcsfs.GCSFileSystem()\n",
        "\n",
        "# Define your bucket and file path in Google Cloud Storage\n",
        "bucket_name = 'okra_foo'  # Replace with your actual bucket name\n",
        "file_name = 'planet_patch_00000000000000000007_20191027T112119_20191027T112939_T28PEV.tif'  # Replace with your actual file name\n",
        "gcs_path = f'gs://{bucket_name}/{file_name}'\n",
        "\n",
        "# Use gcsfs to open the file as a file-like object\n",
        "with gcs.open(gcs_path, 'rb') as f:\n",
        "    # Open the GeoTIFF file using rasterio\n",
        "    with rasterio.open(f) as dataset:\n",
        "        # Read the RGB bands (assuming Band 4 = Red, Band 3 = Green, Band 2 = Blue)\n",
        "        red = dataset.read(4)  # Band 4: Red\n",
        "        green = dataset.read(3)  # Band 3: Green\n",
        "        blue = dataset.read(2)  # Band 2: Blue\n",
        "\n",
        "        rgb_image = np.dstack((red, green, blue))\n",
        "\n",
        "        # Plot the RGB image using matplotlib\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(rgb_image)\n",
        "        plt.title('Sentinel-2 RGB Image from Google Cloud Storage')\n",
        "        plt.axis('off')  # Hide axes\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "HotPMd2eSQhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title False color\n",
        "import gcsfs\n",
        "import rasterio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Google Cloud Storage file system\n",
        "gcs = gcsfs.GCSFileSystem()\n",
        "\n",
        "# Define your bucket and file path in Google Cloud Storage\n",
        "bucket_name = 'okra_foo'  # Replace with your actual bucket name\n",
        "file_name = 'planet_patch_00000000000000000007_20191027T112119_20191027T112939_T28PEV.tif'  # Replace with your actual file name\n",
        "gcs_path = f'gs://{bucket_name}/{file_name}'\n",
        "\n",
        "# Use gcsfs to open the file as a file-like object\n",
        "with gcs.open(gcs_path, 'rb') as f:\n",
        "    # Open the GeoTIFF file using rasterio\n",
        "    with rasterio.open(f) as dataset:\n",
        "        # Check the number of available bands\n",
        "        num_bands = dataset.count\n",
        "        print(f'This GeoTIFF contains {num_bands} bands.')\n",
        "\n",
        "        # Print band descriptions (if available)\n",
        "        for i in range(1, num_bands + 1):\n",
        "            print(f'Band {i}: {dataset.descriptions[i - 1]}')\n",
        "\n",
        "        # Adapt based on the number of available bands\n",
        "        if num_bands >= 4:\n",
        "            # Read the normalized Red, Green, and NIR bands for false color display\n",
        "            # Assuming the file has RGB (bands 1, 2, 3) and NIR (band 4)\n",
        "            red_band = dataset.read(3)  # Read Band 3 (assumed to be Red)\n",
        "            green_band = dataset.read(2)  # Read Band 2 (assumed to be Green)\n",
        "            nir_band = dataset.read(4)  # Read Band 4 (assumed to be NIR)\n",
        "\n",
        "            # Stack bands to create an RGB image for false color (NIR, Red, Green)\n",
        "            false_color_image = np.stack((nir_band, red_band, green_band), axis=-1)\n",
        "\n",
        "            # Display the false color image using matplotlib\n",
        "            plt.figure(figsize=(10, 10))\n",
        "            plt.imshow(false_color_image)\n",
        "            plt.title('Sentinel-2 False Color Image (NIR, Red, Green)')\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"Not enough bands for a false-color image. This GeoTIFF has {num_bands} band(s).\")\n"
      ],
      "metadata": {
        "id": "ydxsk5dPa9tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Compare 2 Patches /RGB\n",
        "import gcsfs\n",
        "import rasterio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Google Cloud Storage file system\n",
        "gcs = gcsfs.GCSFileSystem()\n",
        "\n",
        "# Define your bucket and file paths in Google Cloud Storage for two images\n",
        "bucket_name = 'okra_foo'  # Replace with your actual bucket name\n",
        "file_name_1 = 's2_patch_00000000000000000001_20190811T113329_20190811T113716_T28PCB.tif'  # Replace with the first file name\n",
        "file_name_2 = 'planet_patch_00000000000000000007_20191027T112119_20191027T112939_T28PEV.tif'  # Replace with the second file name\n",
        "gcs_path_1 = f'gs://{bucket_name}/{file_name_1}'\n",
        "gcs_path_2 = f'gs://{bucket_name}/{file_name_2}'\n",
        "\n",
        "# Function to read RGB bands (Red=4, Green=3, Blue=2) from a GeoTIFF in GCS\n",
        "def read_rgb_image(gcs_path):\n",
        "    with gcs.open(gcs_path, 'rb') as f:\n",
        "        with rasterio.open(f) as dataset:\n",
        "            # Read the RGB bands\n",
        "            red = dataset.read(4)  # Band 4: Red\n",
        "            green = dataset.read(3)  # Band 3: Green\n",
        "            blue = dataset.read(2)  # Band 2: Blue\n",
        "            # Stack the bands into a 3D numpy array\n",
        "            rgb_image = np.dstack((red, green, blue))\n",
        "            return rgb_image\n",
        "\n",
        "# Read the two GeoTIFF images\n",
        "rgb_image_1 = read_rgb_image(gcs_path_1)\n",
        "rgb_image_2 = read_rgb_image(gcs_path_2)\n",
        "\n",
        "# Create a figure with two subplots (1 row, 2 columns) to display the images side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "# Display the first image\n",
        "axes[0].imshow(rgb_image_1)\n",
        "axes[0].set_title('Sentinel-2 RGB Image 1')\n",
        "axes[0].axis('off')  # Hide the axes\n",
        "\n",
        "# Display the second image\n",
        "axes[1].imshow(rgb_image_2)\n",
        "axes[1].set_title('Sentinel-2 RGB Image 2')\n",
        "axes[1].axis('off')  # Hide the axes\n",
        "\n",
        "# Show the plot with both images side by side\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HbMzEXpPX0H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title NICFI\n",
        "# Load polygon asset.\n",
        "polygons = ee.FeatureCollection('projects/ee-kkidia3/assets/crop2023/okra')  # simple dataset for trial and error\n",
        "\n",
        "# Define the date range for August, September, and October of 2019.\n",
        "start_date = '2019-08-01'\n",
        "end_date = '2019-10-31'\n",
        "# Load the NICFI/Planet ImageCollection.\n",
        "planet_collection = ee.ImageCollection('projects/planet-nicfi/assets/basemaps/africa') \\\n",
        "    .filterDate(start_date, end_date) \\\n",
        "    .filterBounds(polygons) \\\n",
        "    .select(['R', 'G', 'B', 'N'])  # Red, Green, Blue, NIR bands\n",
        "# Since NICFI/Planet does not provide QA60 cloud mask, we will skip the cloud masking part.\n",
        "def normalize_bands(image):\n",
        "    band_min_max = {\n",
        "        'R': (0, 255),  # Min and max values for Red band\n",
        "        'G': (0, 255),  # Min and max values for Green band\n",
        "        'B': (0, 255),  # Min and max values for Blue band\n",
        "        'N': (0, 255),  # Min and max values for NIR band\n",
        "    }\n",
        "\n",
        "    # Normalize each band (standard min-max scaling).\n",
        "    normalized = ee.Image([\n",
        "        image.select(band).subtract(band_min_max[band][0]).divide(band_min_max[band][1] - band_min_max[band][0])\n",
        "        for band in ['R', 'G', 'B', 'N']\n",
        "    ]).rename(['R', 'G', 'B', 'N'])\n",
        "\n",
        "    return normalized\n",
        "# Apply normalization to the NICFI/Planet image collection\n",
        "processed_collection = planet_collection.map(normalize_bands)\n",
        "def create_fixed_size_patch(feature):\n",
        "    # Get the centroid of the polygon\n",
        "    centroid = feature.geometry().centroid()\n",
        "\n",
        "    # Define the buffer distance as 320 meters to create a 640m x 640m patch\n",
        "    buffer_distance = 320  # Half of the patch size in meters\n",
        "    patch_geometry = centroid.buffer(buffer_distance).bounds()\n",
        "\n",
        "    # Define a consistent UTM projection, adjust UTM zone based on the region (e.g., EPSG:32630 for West Africa)\n",
        "    utm_projection = ee.Projection('EPSG:32630').atScale(4)  # 4-meter resolution for PlanetScope\n",
        "\n",
        "    # Filter images that intersect the patch geometry\n",
        "    images = planet_collection.filterBounds(patch_geometry)\n",
        "\n",
        "    # Apply normalization\n",
        "    images = images.map(normalize_bands)\n",
        "\n",
        "    def clip_and_reproject_image(image):\n",
        "        # Clip the image to the fixed-size geometry (640m x 640m)\n",
        "        clipped_image = image.clip(patch_geometry)\n",
        "\n",
        "        # Reproject to the defined UTM projection with a 4m resolution\n",
        "        reprojected_image = clipped_image.reproject(utm_projection)\n",
        "\n",
        "        # Get the 'system:time_start' property\n",
        "        date = image.get('system:time_start')\n",
        "\n",
        "        # Format the date if available, otherwise use an empty string for the date part\n",
        "        formatted_date = ee.Algorithms.If(date, ee.Date(date).format('YYYY-MM-dd'), '')\n",
        "\n",
        "        # Set metadata to ensure each image has consistent affine transformation and CRS\n",
        "        reprojected_image = reprojected_image.set({\n",
        "            'polygon_id': feature.id(),\n",
        "            'label': feature.get('label'),\n",
        "            'date': formatted_date,  # Use the exact imagery date if available, else ''\n",
        "            'crs': utm_projection.crs(),\n",
        "            'affine': utm_projection.transform(),\n",
        "            'system_index': image.get('system:index')\n",
        "        })\n",
        "\n",
        "        return reprojected_image\n",
        "\n",
        "    # Apply the clipping and reproject function to each image\n",
        "    patches = images.map(clip_and_reproject_image)\n",
        "    return patches\n",
        "\n",
        "# Map the create_fixed_size_patch function over all polygons and flatten the result into an ImageCollection\n",
        "all_patches = polygons.map(create_fixed_size_patch).flatten()\n",
        "all_patches = ee.ImageCollection(all_patches)\n",
        "\n",
        "# Export the patches with appropriate parameters for NICFI/Planet imagery\n",
        "def export_fixed_patch(index):\n",
        "    image = ee.Image(patches_list.get(index))\n",
        "    polygon_id = image.get('polygon_id').getInfo()\n",
        "    date = image.get('date').getInfo() or \"\"  # If no date, use an empty string\n",
        "    system_index = image.get('system_index').getInfo()\n",
        "\n",
        "    filename = f'planet_patch_{polygon_id}{\"_\" + date if date else \"\"}_{system_index}'\n",
        "\n",
        "    task = ee.batch.Export.image.toCloudStorage(\n",
        "        image=image.toFloat(),\n",
        "        description=filename,\n",
        "        bucket='okra_foo',  # your bucket name\n",
        "        scale=4,  # 4-meter resolution for PlanetScope imagery\n",
        "        region=image.geometry().bounds().getInfo()['coordinates'],  # Explicitly set region\n",
        "        crs='EPSG:32630',  # Consistent UTM projection\n",
        "        fileFormat='GeoTIFF',\n",
        "        formatOptions={\n",
        "            'cloudOptimized': True  # Optimized for cloud storage\n",
        "        }\n",
        "    )\n",
        "    task.start()\n",
        "    print(f'Exporting {filename} as GeoTIFF')\n"
      ],
      "metadata": {
        "id": "6R13aR6bqfBU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over the patches and export each one in batches\n",
        "batch_size = 10  # Adjust batch size as needed\n",
        "for i in range(0, patch_count, batch_size):\n",
        "    for j in range(i, min(i + batch_size, patch_count)):\n",
        "        export_fixed_patch(j)\n",
        "    print(f'Batch {i // batch_size + 1} exported.')"
      ],
      "metadata": {
        "id": "NqDZm7tGFXc7"
      },
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}