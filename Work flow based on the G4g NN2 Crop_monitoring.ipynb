{"cells":[{"cell_type":"code","source":["!pip uninstall python-snappy"],"metadata":{"id":"-2HNgdYTSFTE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install apache_beam"],"metadata":{"id":"87fdWVDySE7C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install apache-beam[gcp]"],"metadata":{"id":"MsEu_zyxFYI_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import apache_beam as beam\n","from apache_beam.options.pipeline_options import PipelineOptions"],"metadata":{"id":"7HmgAoliM2Tk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oe2js4BMPc9W"},"source":["### What resource we need to Train this large data??: GPU"]},{"cell_type":"markdown","metadata":{"id":"1Z6Vn4bqNvLA"},"source":["### Completed Tasks:\n","\n","--> GEE Asset/Crop polygons\n","\n","--> polygon area estimation for each crop type data\n","\n","--> Plante/NICFI Pixel count for each crops, each polygons\n","\n","--> Graphic representation for area vs Pixel count"]},{"cell_type":"markdown","metadata":{"id":"o0EPLwkbNSX_"},"source":["### Next tasks:\n","\n","--> Cloud masking for Planet (Algorthms)\n","\n","--> Image Normalization: Scale the pixel values (e.g., 0-255) to a range that is typical for neural network inputs, such as 0-1 or -1 to 1.\n","\n","--> Data Augmentation: To increase the diversity of the training data and prevent overfitting, apply transformations like rotations, translations, scaling, and horizontal flipping.\n","\n","--> Patch Extraction: For high-resolution images, it might be necessary to create smaller, manageable patches. This makes the training process more efficient and helps in handling large images during deployment.\n","\n","--> DL (FCNN) Trainings\n","--> Parallel ML training"]},{"cell_type":"markdown","metadata":{"id":"6uHyL3LNf6f7"},"source":["### Reading corner about crop mapping in Senegal\n","SEN4STAT/ESA: https://www.esa-sen4stat.org/user-stories/senegal-prototype/\n","\n","EOSTAT/FAO: https://data.apps.fao.org/catalog/dataset/5c377b2b-3c2e-4b70-afd7-0c80900b68bb/resource/50bc9ff5-95d2-40cd-af12-6aee2cfcc4ae"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0aaqStUK6zab"},"outputs":[],"source":["#Lib imports:\n","import ee\n","#print('Using EE version ', ee.__version__)\n","import folium\n","#print('Using Folium version ', folium.__version__)\n","from os import MFD_HUGE_1MB\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","#print('Using TF version ', tf.__version__)\n","from typing import Dict, Iterable, List, Tuple\n","#from google.colab import auth"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KcKR75Me7D6G"},"outputs":[],"source":["# authenticate with user credentials\n","#auth.authenticate_user()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tKVguRub1wd_"},"outputs":[],"source":["# Authenticate to the Earth Engine servers\n","ee.Authenticate()\n","\n","# Initialize the Earth Engine object with Google Cloud project ID\n","project_id = 'ee-kkidia3'\n","ee.Initialize(project=project_id)\n"]},{"cell_type":"code","source":["# Initialize the GEE module.\n","ee.Initialize()"],"metadata":{"id":"F0HGFmampJKD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NHmwEttlt5TV","cellView":"form"},"outputs":[],"source":["# @title Default title text\n","\n","def list_ee_assets(folder_path):\n","    \"\"\" List all assets in the specified Earth Engine folder path \"\"\"\n","    try:\n","        assets_list = ee.data.getList({'id': folder_path})\n","        for asset in assets_list:\n","            print(asset['id'])\n","    except Exception as e:\n","        print(\"Error accessing Earth Engine assets:\", e)\n","\n","# GEE Asset path\n","#folder_path = 'users/kkidia3'\n","\n","# Call the function to list assets\n","#list_ee_assets(folder_path)\n"]},{"cell_type":"markdown","source":["### Crop fields polygon for rainfed season 2018-2020"],"metadata":{"id":"onPAzxk-kipr"}},{"cell_type":"code","source":["#Data base from 2018-2020: Crop field plot polygon.In the dataset different crop types assembeled in one file.\n","data_2018 = ee.FeatureCollection(\"projects/ee-djitastar/assets/data_2018\")\n","data_2019 = ee.FeatureCollection(\"projects/ee-djitastar/assets/data_2019\")\n","data_2020 = ee.FeatureCollection(\"projects/ee-djitastar/assets/data_2020\")\n","\n","#merge for the rainfed croping season 2018-2020.\n","merge18_20 = data_2018.merge(data_2019).merge(data_2020)"],"metadata":{"id":"0XkdpVMek6P4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["attribute table of data"],"metadata":{"id":"FGvg1JYHUAvx"}},{"cell_type":"code","source":["from shapely.geometry import shape\n","import geopandas as gpd\n","\n","# Function to convert Earth Engine FeatureCollection to a pandas DataFrame\n","def fc_to_df(fc):\n","    features = fc.getInfo()['features']\n","    dict_list = [f['properties'] for f in features]\n","    for d, f in zip(dict_list, features):\n","        d['geometry'] = shape(f['geometry'])\n","    return pd.DataFrame(dict_list)\n","\n","# Convert the entire feature collection to a pandas DataFrame\n","df = fc_to_df(data_2018)\n","print(df.head())\n","\n","# to convert this DataFrame to a GeoDataFrame\n","gdf = gpd.GeoDataFrame(df, geometry='geometry')\n","print(gdf.head())\n","\n","# save the DataFrame to a CSV file if needed\n","#df.to_csv('data_2018_features.csv', index=False)\n","#print(\"Feature table saved to data_2018_features.csv\")\n"],"metadata":{"id":"SL1hiwutQ92E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualize 18-20"],"metadata":{"id":"RfSBSyqWoGCi"}},{"cell_type":"code","source":["# @title Default title text\n","# Define a function to add Earth Engine vector data as a layer to a Folium map\n","def add_ee_vector_layer(feature_collection, style, layer_name, map_object):\n","    painted = ee.Image().paint(feature_collection, 'constant', 2)  # Here 'constant' is a dummy property for visualization\n","    map_id_dict = painted.getMapId(style)\n","    folium.TileLayer(\n","        tiles=map_id_dict['tile_fetcher'].url_format,\n","        attr='Map Data &copy; Google Earth Engine',\n","        name=layer_name,\n","        overlay=True,\n","        control=True\n","    ).add_to(map_object)\n","\n","# Create a Folium map object\n","center = [14.4974, -14.4524]  # Center of the map (e.g., Senegal)\n","m1 = folium.Map(location=center, zoom_start=7)\n","\n","# Styling for the vector layer\n","style = {\n","    'color': 'blue',  # Line color\n","    'fillColor': '00000000',  # Fill color with opacity (00)\n","}\n","\n","# Add the merged crop fields to the map\n","add_ee_vector_layer(merge18_20, style, 'Merged Crops 2019-2020', m1)\n","\n","# Add a layer control panel to the map\n","folium.LayerControl().add_to(m1)\n","\n","# Display the map\n","m1\n"],"metadata":{"cellView":"form","id":"SDtRO5zDlYo9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data_2018.getInfo())"],"metadata":{"id":"5Q4xwx4noW20"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Polygons for FY2023"],"metadata":{"id":"tVWi-cw_kcn7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"p6VLDp2rv8JE"},"outputs":[],"source":["# Function to load individual crop feature collections\n","def load_crop_data(asset_id):\n","    return ee.FeatureCollection(asset_id)\n","\n","# Load individual rainfed 2023 crop feature collections\n","gnut = load_crop_data('projects/ee-kkidia3/assets/groundnut')\n","gsorrel = load_crop_data('projects/ee-kkidia3/assets/guinea_sorrel')\n","rice = load_crop_data('projects/ee-kkidia3/assets/rice')\n","sesame = load_crop_data('projects/ee-kkidia3/assets/sesame')\n","soye = load_crop_data('projects/ee-kkidia3/assets/soye')\n","watermelon = load_crop_data('projects/ee-kkidia3/assets/watermelon')\n","cassava = load_crop_data('projects/ee-kkidia3/assets/cassava')\n","gnutmix = load_crop_data('projects/ee-kkidia3/assets/groundnut_mixed')\n","milletmix = load_crop_data('projects/ee-kkidia3/assets/millet_mixed')\n","okra = load_crop_data('projects/ee-kkidia3/assets/okra')\n","sorgh = load_crop_data('projects/ee-kkidia3/assets/sorgh')\n","wheat = load_crop_data('projects/ee-kkidia3/assets/wheat')\n","millet = load_crop_data('projects/ee-kkidia3/assets/millet')\n","cowpmix = load_crop_data('projects/ee-kkidia3/assets/cowpea_mixed')\n","fallow = load_crop_data('projects/ee-kkidia3/assets/fellow')\n","\n","# Merge all crop fields into a single feature collection\n","merged2023 = gnut.merge(gsorrel).merge(rice).merge(sesame).merge(soye).merge(watermelon)\\\n","                .merge(cassava).merge(gnutmix).merge(milletmix).merge(okra).merge(sorgh)\\\n","                .merge(wheat).merge(millet).merge(cowpmix).merge(fallow)\n","\n"]},{"cell_type":"code","source":["print(gnut.getInfo())"],"metadata":{"id":"6XMwutmUpGRk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Select sample 2 polygons from goundnet polygons"],"metadata":{"id":"xDbpq60yag0J"}},{"cell_type":"code","source":["#print(poly_2)\n","# @title Default title text\n","# Select two specific polygons by their indices or properties. e.g gnut\n","# Here, I want to select the first two polygons.\n","poly_2 = gnut.toList(2)\n","# Get the individual polygons\n","polygon1 = ee.Feature(poly_2.get(0))\n","polygon2 = ee.Feature(poly_2.get(1))\n","\n","# Print the geometries of the selected polygons to verify.\n","print('Polygon 1_Blue:', polygon1.geometry().getInfo())\n","print('Polygon 2_Red:', polygon2.geometry().getInfo())\n","# Create a map centered at an arbitrary point @polygon1\n","map_center = [13.237198228125724,-14.715474917616827]  #set this to a blue selected @polygon1\n","m = folium.Map(location=map_center, zoom_start=30) #\n","\n","# Define a function to add a feature to the folium map.\n","def add_feature_to_map(feature, map_obj, color):\n","    geom = feature.geometry().getInfo()\n","    coords = geom['coordinates']\n","    if geom['type'] == 'Polygon':\n","        folium.Polygon(locations=[(pt[1], pt[0]) for pt in coords[0]], color=color).add_to(map_obj)\n","    elif geom['type'] == 'MultiPolygon':\n","        for poly in coords:\n","            folium.Polygon(locations=[(pt[1], pt[0]) for pt in poly[0]], color=color).add_to(map_obj)\n","\n","# Add polygons to the map\n","add_feature_to_map(polygon1, m, 'blue')\n","add_feature_to_map(polygon2, m, 'red')\n","\n","# Display the map\n","#m\n"],"metadata":{"id":"jHg2HndmK3tK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the time range\n","START = '2023-09-01'\n","END = '2023-09-30'\n","\n","# Polygon coordinates\n","COORDS = [[-14.715474917616827, 13.237198228125724],\n","          [-14.714971029320179, 13.237131335847982],\n","          [-14.714841720208264, 13.237135818148086],\n","          [-14.714864057837882, 13.237282954362941],\n","          [-14.714921993298699, 13.237389931092336],\n","          [-14.714756997435309, 13.237452415652427],\n","          [-14.714890756204218, 13.23747022665853],\n","          [-14.71482392130252, 13.237563840257419],\n","          [-14.714792684227906, 13.237653054642704],\n","          [-14.714663375265559, 13.23760402506512],\n","          [-14.714667824987172, 13.237737716223846],\n","          [-14.714556315002342, 13.237751104481083],\n","          [-14.714435994662244, 13.23802313618169],\n","          [-14.714774885327822, 13.238223778014577],\n","          [-14.714971029320179, 13.238299595698706],\n","          [-14.715474917616827, 13.237198228125724]] #this coordinate is based on polygon1 (groundnut)\n","\n","# Convert coordinates to ee.Geometry\n","TEST_ROI = ee.Geometry.Polygon([COORDS])\n","\n","PATCH_SIZE = 128  # Pixels\n","SCALE = 10        # Meters per pixel for Sentinel\n","SCALE2 = 4.47  #Planet\n","\n","# Pre-compute a geographic coordinate system.\n","proj = ee.Projection('EPSG:4326').atScale(SCALE).getInfo()\n","\n","# Get scales out of the transform.\n","SCALE_X = proj['transform'][0]\n","SCALE_Y = -proj['transform'][4]\n","\n","PATCH_SIZE = 128\n","OFFSET_X = -SCALE_X * PATCH_SIZE / 2\n","OFFSET_Y = -SCALE_Y * PATCH_SIZE / 2\n","\n","REQUEST = {\n","    'fileFormat': 'NPY',\n","    'grid': {\n","        'dimensions': {\n","            'width': PATCH_SIZE,\n","            'height': PATCH_SIZE\n","        },\n","        'affineTransform': {\n","            'scaleX': SCALE_X,\n","            'shearX': 0,\n","            'shearY': 0,\n","            'scaleY': SCALE_Y,\n","            'translateX': OFFSET_X,\n","            'translateY': OFFSET_Y\n","        },\n","        'crsCode': proj['crs']\n","    }\n","}\n","\n","VALIDATION_RATIO = 0.2\n","POINTS_PER_CLASS = 64\n","TEST_POINT = ee.Geometry.Point(COORDS[0])\n","\n","# The TEST_ROI is precisely 16 times the size of an individual patch.\n","TEST_ROI = TEST_POINT.buffer(2 * PATCH_SIZE * SCALE, maxError=1).bounds(maxError=1)\n","\n","# A list of regions from which to sample training data.\n","REGIONS = merged2023\n","# Predictors.\n","INPUT_BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12'] #sentinel\n","INPUT_BANDS2 = ['R', 'G', 'B'] #planet\n","# Target variable.\n","OUTPUT_BANDS = ['landcover']\n","# Input stack.\n","BANDS = INPUT_BANDS + OUTPUT_BANDS\n","# Map from input name to a fixed length feature.\n","FEATURES_DICT = {\n","    k: tf.io.FixedLenFeature(shape=[PATCH_SIZE, PATCH_SIZE], dtype=tf.float32)\n","    for k in BANDS\n","}\n","\n","CLASSIFICATIONS = {\n","    \"💧 Water\":              \"419BDF\",\n","    \"🌳 Trees\":              \"397D49\",\n","    \"🌾 Grass\":              \"88B053\",\n","    \"🌿 Flooded vegetation\": \"7A87C6\",\n","    \"🚜 Crops\":              \"E49635\",\n","    \"🪴 Shrub and scrub\":    \"DFC35A\",\n","    \"🏗️ Built-up areas\":     \"C4281B\",\n","    \"🪨 Bare ground\":        \"A59B8F\",\n","    \"❄️ Snow and ice\":       \"B39FE1\",\n","}\n"],"metadata":{"id":"s5pnGCKJPKf_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Classifieres for crop type"],"metadata":{"id":"onwJJvHz0GwX"}},{"cell_type":"code","source":["import matplotlib.patches as mpatches\n","# Define a color map for each crop type\n","color_map = {\n","    'groundnut': '#FF6347',  # Tomato red\n","    'guinea_sorrel': '#FFD700',  # Gold\n","    'rice': '#32CD32',  # Lime Green\n","    'sesame': '#8A2BE2',  # Blue Violet\n","    'soye': '#FF4500',  # Orange Red\n","    'watermelon': '#00FA9A',  # Medium Spring Green\n","    'cassava': '#7B68EE',  # Medium Slate Blue\n","    'groundnut_mixed': '#D2691E',  # Chocolate\n","    'millet_mixed': '#DAA520',  # Golden Rod\n","    'okra': '#ADFF2F',  # Green Yellow\n","    'sorgh': '#FF00FF',  # Magenta\n","    'wheat': '#F4A460',  # Sandy Brown\n","    'millet': '#00CED1',  # Dark Turquoise\n","    'cowpea_mixed': '#FF1493',  # Deep Pink\n","    'fallow': '#708090'   # Slate Gray\n","}\n","\n","# Create legend patches\n","legend_patches = [mpatches.Patch(color=color, label=key) for key, color in color_map.items()]\n","\n","# Create the legend\n","plt.figure(figsize=(5, 5))\n","plt.legend(handles=legend_patches, loc='center', title=\"Crop Legend\", ncol=2)\n","plt.axis('off')  # Hide axes\n","plt.show()\n"],"metadata":{"id":"mCOK14LuzjkI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["for Worldcover and dynmaic world"],"metadata":{"id":"_q7pm8Ua1A1t"}},{"cell_type":"code","source":["def display_legend():\n","  reset_color = \"\\u001b[0m\"\n","  colored = lambda red, green, blue: f\"\\033[48;2;{red};{green};{blue}m\"\n","  for name, color in CLASSIFICATIONS.items():\n","    red   = int(color[0:2], 16)\n","    green = int(color[2:4], 16)\n","    blue  = int(color[4:6], 16)\n","    print(f\"{colored(red, green, blue)}   {reset_color} {name}\")\n","\n","display_legend()"],"metadata":{"id":"JxDUDsXVQieg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compare the location based on DYNAMICWORLD"],"metadata":{"id":"ZXjgXjzDcLTq"}},{"cell_type":"code","source":["# @title Default title text\n","image = (\n","    ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1')\n","    .filterDate(START, END)\n","    .median()\n",")\n","\n","vis_params = {\n","  'max': len(CLASSIFICATIONS) - 1,\n","  'palette': list(CLASSIFICATIONS.values()),\n","  'bands': ['label'],\n","}\n","folium.Map(\n","    location=(COORDS[1], COORDS[0]),\n","    zoom_start=10,\n","    tiles=image.getMapId(vis_params)[\"tile_fetcher\"].url_format,\n","    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",")"],"metadata":{"id":"Uk4ntirMQskU","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sentinel-2 surface reflectance\n","\n","We will use a cloud-freee composite of Sentinel-2 surface reflectance data for predictors.  See [the Code Editor example](https://code.earthengine.google.com/?scriptPath=Examples%3ACloud%20Masking%2FSentinel2%20Cloud%20And%20Shadow) for details."],"metadata":{"id":"yLNq7Dkkcukq"}},{"cell_type":"markdown","source":["SENTINEL"],"metadata":{"id":"5CyqC6AahMTG"}},{"cell_type":"code","source":["def get_s2_composite(roi, start, end):\n","  s2c = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n","  s2sr = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n","  s2c = s2c.filterBounds(roi.buffer(100*1000, 1000)).filterDate(start, end)\n","  s2sr = s2sr.filterBounds(roi.buffer(100*1000, 1000)).filterDate(start, end)\n","\n","  def index_join(collection_a, collection_b, property_name):\n","    joined = ee.ImageCollection(\n","        ee.Join.saveFirst(property_name).apply(\n","            primary=collection_a,\n","            secondary=collection_b,\n","            condition=ee.Filter.equals(\n","                leftField='system:index',\n","                rightField='system:index')))\n","    return joined.map(\n","        lambda image: image.addBands(ee.Image(image.get(property_name))))\n","\n","  def mask_image(image):\n","    prob = image.select('probability')\n","    return image.select('B.*').divide(10000).updateMask(prob.lt(50))\n","\n","  with_cloud_probability = index_join(s2sr, s2c, 'cloud_probability')\n","  masked = ee.ImageCollection(with_cloud_probability.map(mask_image))\n","  return masked.select(INPUT_BANDS).median().float().unmask(0)\n","\n","image = get_s2_composite(TEST_POINT, START, END)\n","\n","vis_params = {\n","  'min': 0,\n","  'max': 0.4,\n","  'bands': ['B4', 'B3', 'B2'],\n","}\n","\n","folium.Map(\n","    location=(13.237198228125724, -14.715474917616827), #Senegal\n","    zoom_start=10,\n","    tiles=image.getMapId(vis_params)[\"tile_fetcher\"].url_format,\n","    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",")\n","\n"],"metadata":{"id":"vDyYGImzSboY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["NICFI/PLANET"],"metadata":{"id":"scSiuHvjgjRY"}},{"cell_type":"code","source":["\n","# Define the region of interest (ROI)\n","TEST_POINT = ee.Geometry.Point([-14.715474917616827, 13.237198228125724])\n","\n","def get_planet_composite(roi, start, end):\n","    # Adjust the dataset references for Planet/NICFI\n","    planet_ic = ee.ImageCollection('projects/planet-nicfi/assets/basemaps/africa')\n","\n","    # Filter by date and region\n","    planet_ic = planet_ic.filterBounds(roi.buffer(100*1000, 1000)).filterDate(start, end)\n","\n","    # Planet NICFI doesn't have cloud probability bands, we will assume all images are usable\n","    def mask_image(image):\n","        return image.select(['R', 'G', 'B']).divide(10000)  # Adjust bands as needed\n","\n","    masked = planet_ic.map(mask_image)\n","    return masked.median().float().unmask(0)\n","\n","image = get_planet_composite(TEST_POINT, START, END)\n","\n","# Visualization parameters for Planet NICFI\n","vis_params = {\n","    'min': 0,\n","    'max': 0.4,\n","    'bands': ['G', 'R', 'B'],  # Adjust the bands as needed\n","}\n","\n","# Define a method for displaying Earth Engine image tiles with Folium.\n","def add_ee_layer(self, ee_image_object, vis_params, name):\n","    map_id_dict = ee.Image(ee_image_object).getMapId(vis_params)\n","    folium.raster_layers.TileLayer(\n","        tiles=map_id_dict['tile_fetcher'].url_format,\n","        attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n","        name=name,\n","        overlay=True,\n","        control=True\n","    ).add_to(self)\n","\n","# Add EE drawing method to folium.\n","folium.Map.add_ee_layer = add_ee_layer\n","\n","# Create a folium map and add the image layer\n","map = folium.Map(\n","    location=[13.237198228125724, -14.715474917616827],  # Senegal\n","    zoom_start=10,\n",")\n","\n","# Add the Planet NICFI layer to the map\n","map.add_ee_layer(image, vis_params, 'Planet NICFI Composite')\n","\n","# Add a layer control panel to the map.\n","map.add_child(folium.LayerControl())\n","\n","# Display the map\n","map\n","\n","\n"],"metadata":{"id":"z3VtrBLQcYFC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compare the sample location to ESA worldcover. This is noting to do with the training"],"metadata":{"id":"5e2AavVDWSuV"}},{"cell_type":"code","source":["def landcover_image() -> ee.Image:\n","    # Remap the ESA classifications into the Dynamic World classifications\n","    fromValues = [10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 100]\n","    toValues = [1, 5, 2, 4, 6, 7, 8, 0, 3, 3, 7]\n","    return (\n","        ee.ImageCollection('ESA/WorldCover/v200')\n","        .first()\n","        .select('Map')\n","        .remap(fromValues, toValues)\n","        .rename(OUTPUT_BANDS)\n","    )\n","\n","image = landcover_image()\n","\n","vis_params = {\n","  'bands': OUTPUT_BANDS,\n","  'palette': list(color_map.values()), #CLASSIFICATION lenged as an option\n","}\n","\n","folium.Map(\n","    location=(13.237198228125724, -14.715474917616827),\n","    zoom_start=10,\n","    tiles=image.getMapId(vis_params)[\"tile_fetcher\"].url_format,\n","    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",")"],"metadata":{"id":"nanqjTgxTT0B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.api_core import exceptions, retry\n","import io\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import requests\n","from typing import Dict, Iterable, List, Tuple\n","\n","\n","def sample_random_points(region: ee.Geometry, points_per_class: int) -> Iterable[List]:\n","  \"\"\"Get a generator of random points in the region.\"\"\"\n","  image = landcover_image().select(OUTPUT_BANDS).int()\n","  points = image.stratifiedSample(\n","      points_per_class,\n","      region=region,\n","      scale=SCALE,\n","      geometries=True,\n","  )\n","  for point in points.toList(points.size()).getInfo():\n","      yield point['geometry']['coordinates']\n","\n","\n","@retry.Retry()\n","def get_training_patch(coords: List[float]) -> np.ndarray:\n","  \"\"\"Get a training patch centered on the coordinates.\"\"\"\n","  point = ee.Geometry.Point(coords)\n","  image = get_s2_composite(point, START, END).addBands(landcover_image())\n","  request = dict(REQUEST)\n","  request['expression'] = image\n","  request['grid']['affineTransform']['translateX'] = coords[0] + OFFSET_X\n","  request['grid']['affineTransform']['translateY'] = coords[1] + OFFSET_Y\n","  return np.load(io.BytesIO(ee.data.computePixels(request)))\n"],"metadata":{"id":"Be4O0ibWTx2O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample = sample_random_points(TEST_ROI, POINTS_PER_CLASS)\n","coords = next(sample)\n","print(f\"coords: {coords}\")\n","\n","patch = get_training_patch(coords)\n","print(f\"patch shape={patch.shape} bands={len(patch.dtype)}\")\n","print(f\"dtype: {patch.dtype}\")"],"metadata":{"id":"TErR2PzzUIJa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Render the test patch to check the predictors\n","NOTE: This is the fake legend the the training legend should be based on crop types class"],"metadata":{"id":"oqYRl4B_XJh4"}},{"cell_type":"code","source":["def render_rgb_images(values: np.ndarray, min=0.0, max=1.0) -> np.ndarray:\n","  scaled_values = (values - min) / (max - min)\n","  rgb_values = scaled_values * 255\n","  return rgb_values.astype(np.uint8)\n","\n","def render_sentinel2(patch: np.ndarray, min=0.0, max=0.3) -> np.ndarray:\n","  red   = patch[\"B4\"]\n","  green = patch[\"B3\"]\n","  blue  = patch[\"B2\"]\n","  rgb_patch = np.stack([red, green, blue], axis=-1)\n","  return render_rgb_images(rgb_patch, min, max)\n","\n","plt.imshow(render_sentinel2(patch))"],"metadata":{"id":"9OhmMY5OUTDT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(patch)"],"metadata":{"id":"fNwjYae9UtYS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def render_classifications(values: np.ndarray, palette: List[str]) -> np.ndarray:\n","  # Create a color map from a hex color palette.\n","  xs = np.linspace(0, len(palette), 256)\n","  indices = np.arange(len(palette))\n","  color_map = np.array([\n","        np.interp(xs, indices, [int(c[0:2], 16) for c in palette]),  # red\n","        np.interp(xs, indices, [int(c[2:4], 16) for c in palette]),  # green\n","        np.interp(xs, indices, [int(c[4:6], 16) for c in palette]),  # blue\n","  ]).astype(np.uint8).transpose()\n","\n","  color_indices = (values / len(palette) * 255).astype(np.uint8)\n","  return np.take(color_map, color_indices, axis=0)\n","\n","def render_landcover(patch: np.ndarray) -> np.ndarray:\n","  return render_classifications(patch, list(CLASSIFICATIONS.values()))\n","\n","landcover_rgb = render_landcover(patch[\"landcover\"])\n","\n","print(f\"landcover_rgb: {landcover_rgb.dtype} {landcover_rgb.shape}\")\n","plt.imshow(landcover_rgb)\n","display_legend()"],"metadata":{"id":"fl6yCj7WU7LL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def serialize(patch: np.ndarray) -> bytes:\n","  \"\"\"Serializes a patch of data into a tf.train.Example proto.\"\"\"\n","  features = {\n","      name: tf.train.Feature(\n","          float_list=tf.train.FloatList(value=patch[name].flatten())\n","      )\n","      for name in patch.dtype.names\n","  }\n","  example = tf.train.Example(features=tf.train.Features(feature=features))\n","  return example.SerializeToString()\n","\n","serialized = serialize(patch)\n","print(f\"serialized: {type(serialized).__name__} ({len(serialized)})\")"],"metadata":{"id":"yVKw0TcFDwzW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","def split_dataset(element, num_partitions: int) -> int:\n","  weights = [1 - VALIDATION_RATIO, VALIDATION_RATIO]\n","  return random.choices([0, 1], weights)[0]\n","\n","beam_options = PipelineOptions(\n","    [], direct_num_workers=len(REGIONS), direct_running_mode='multi_threading')\n","\n","with beam.Pipeline(options=beam_options) as pipeline:\n","  training_data, validation_data = (\n","      pipeline\n","      | \"Create regions\" >> beam.Create(REGIONS)\n","      | \"Sample random points\" >> beam.FlatMap(sample_random_points, POINTS_PER_CLASS)\n","      | \"Get patch\" >> beam.Map(get_training_patch)\n","      | \"Serialize\" >> beam.Map(serialize)\n","      | \"Split dataset\" >> beam.Partition(split_dataset, 2)\n","  )\n","\n","  training_data | \"Write training data\" >> beam.io.WriteToTFRecord(\n","      f\"gs://{BUCKET}/crop-2023-datasets/training\", file_name_suffix=\".tfrecord.gz\"\n","  )\n","  validation_data | \"Write validation data\" >> beam.io.WriteToTFRecord(\n","      f\"gs://{BUCKET}/crop-2023-datasets/validation\", file_name_suffix=\".tfrecord.gz\"\n","  )"],"metadata":{"id":"M4HaJHs_iWw8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VVshRcNt4XOY"},"source":["### Visulaize the polygons in map"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WYyHLwnX4VrV","cellView":"form"},"outputs":[],"source":["# @title Default title text\n","# Define a function to add Earth Engine vector data as a layer to a Folium map\n","def add_ee_vector_layer(feature_collection, style, layer_name, map_object):\n","    painted = ee.Image().paint(feature_collection, 'constant', 2)  # Here 'constant' is a dummy property for visualization\n","    map_id_dict = painted.getMapId(style)\n","    folium.TileLayer(\n","        tiles=map_id_dict['tile_fetcher'].url_format,\n","        attr='Map Data &copy; Google Earth Engine',\n","        name=layer_name,\n","        overlay=True,\n","        control=True\n","    ).add_to(map_object)\n","\n","# Create a Folium map object\n","center = [14.4974, -14.4524]  # Center of the map in  Senegal\n","m = folium.Map(location=center, zoom_start=7)\n","\n","# Styling for the vector layer\n","style = {\n","    'color': 'blue',  # Line color\n","    'fillColor': '00000000',  # Fill color with opacity (00)\n","}\n","\n","# Add the merged crop fields to the map\n","add_ee_vector_layer(merged2023, style, 'Merged Crops 2023', m)\n","\n","# Add a layer control panel to the map\n","folium.LayerControl().add_to(m)\n","\n","# Display the map\n","#m"]},{"cell_type":"markdown","metadata":{"id":"BbkKpj3Gjakc"},"source":["Remove overlapping polygons from your merged feature collection. Overlaps is due to data collection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ws1P-8uR2qas"},"outputs":[],"source":["# Identify and remove overlapping polygons\n","def remove_overlaps(feature_collection):\n","    non_overlapping = ee.FeatureCollection([])\n","    while True:\n","        # Find the first feature\n","        first = feature_collection.first()\n","        if first is None:\n","            break\n","        first = ee.Feature(first)\n","\n","        # Add the first feature to the non-overlapping collection\n","        non_overlapping = non_overlapping.merge(ee.FeatureCollection([first]))\n","\n","        # Remove features that overlap with the first feature\n","        feature_collection = feature_collection.filter(ee.Filter.disjoint('.geo', first.geometry()))\n","\n","    return non_overlapping\n","\n","# Apply the function to remove overlaps\n","merged2023_non_overlapping = remove_overlaps(merged2023)\n","\n","# Print the result (optional, for verification purposes)\n","print(merged2023_non_overlapping.size().getInfo())"]},{"cell_type":"markdown","metadata":{"id":"2cyMHrZPxdQr"},"source":["For example let me sort out only two polygons out of maney gnut polygons for closer look and simple analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-UXDK9RywbX3","cellView":"code"},"outputs":[],"source":["# @title Default title text\n","# Select two specific polygons by their indices or properties. e.g gnut\n","# Here, I want to select the first two polygons.\n","poly_2 = gnut.toList(2)\n","\n","#print(poly_2)\n","\n","# Get the individual polygons\n","polygon1 = ee.Feature(poly_2.get(0))\n","polygon2 = ee.Feature(poly_2.get(1))\n","\n","# Print the geometries of the selected polygons to verify.\n","print('Polygon 1_Blue:', polygon1.geometry().getInfo())\n","print('Polygon 2_Red:', polygon2.geometry().getInfo())\n","\n","\n","# Create a map centered at an arbitrary point @polygon1\n","map_center = [13.237198228125724,-14.715474917616827]  #set this to a blue selected @polygon1\n","m = folium.Map(location=map_center, zoom_start=30) #\n","\n","# Define a function to add a feature to the folium map.\n","def add_feature_to_map(feature, map_obj, color):\n","    geom = feature.geometry().getInfo()\n","    coords = geom['coordinates']\n","    if geom['type'] == 'Polygon':\n","        folium.Polygon(locations=[(pt[1], pt[0]) for pt in coords[0]], color=color).add_to(map_obj)\n","    elif geom['type'] == 'MultiPolygon':\n","        for poly in coords:\n","            folium.Polygon(locations=[(pt[1], pt[0]) for pt in poly[0]], color=color).add_to(map_obj)\n","\n","# Add polygons to the map\n","add_feature_to_map(polygon1, m, 'blue')\n","add_feature_to_map(polygon2, m, 'red')\n","\n","# Display the map\n","m"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8RRCWAtzMgXZ"},"outputs":[],"source":["# Calculate the total area of polygons in square meters for Rainfed season 2023. Example groundnut\n","area = gnut.geometry().area().format('%.1f').getInfo()\n","print(f\"Area M2: {area} square meters OR,\")\n","\n","# Calculate the area in hectares (1 hectare = 10,000 square meters)\n","area_ha = gnut.geometry().area().divide(10000).format('%.1f').getInfo() # Convert square meters to hectares\n","print(f\"Area Ha: {area_ha} hectares\")\n","\n","# Calculate the perimeter in meters\n","perimeter = gnut.geometry().perimeter().format('%.1f').getInfo()\n","print(f\"Perimeter: {perimeter} meters\")\n","\n","# Get the centroid of the polygon\n","centroid = gnut.geometry().centroid().getInfo()\n","print(f\"Centroid: {centroid}\")\n","\n","# Get the bounding box as a GeoJSON\n","bounding_box = gnut.geometry().bounds().getInfo()\n","print(f\"Bounding Box: {bounding_box}\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rn2qJaFPJhVb"},"source":["### Estimate the each polygon area (m2), for each crop e.g Groundnt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BZ4Dh3yNIChw"},"outputs":[],"source":["# Define a function to calculate the area of each polygon in hectares\n","#def calc_area(feature):\n","    #return feature.set('area_ha', ee.Number(feature.geometry().area().divide(10000)).format('%.3f'))\n","\n","# Define a function to calculate the area of each polygon in m2\n","def calc_area(feature):\n","  return feature.set('area_m2', ee.Number(feature.geometry().area())\n","  .format('%.1f'))#.divide(10000)) & # .format('%.1f') use to limit the digits\n","\n","# Apply the function to each feature in the collection\n","area_mapped = merged2023.map(calc_area)\n","\n","# Fetch the mapped area information from the server\n","areas_info = area_mapped.limit(10).getInfo()  # Limiting to the first 10 features directly to show everthing remove .limit(10)\n","\n","# Iterate through the first 10 features and print area in hectares with 3 decimal places\n","for feature in areas_info['features']:\n","    area = feature['properties']['area_m2'] ##m2\n","    print(f\"Gnut Polygon_Area: {area} m2\") #m2\n"]},{"cell_type":"markdown","metadata":{"id":"hDwoYgNvPUjs"},"source":["### Area (ha) distributions e.g Groundnut"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bj9r9SdyNQOL"},"outputs":[],"source":["# Define a function to calculate the area of each polygon in hectares\n","#def calc_area(feature):\n","    #return feature.set('area_ha', feature.geometry().area().divide(10000))  # Convert from square meters to hectares\n","\n","# Define a function to calculate the area of each polygon in hectares\n","def calc_area(feature):\n","    return feature.set('area_ha', feature.geometry().area().divide(10000)) #--> # Convert from square meters to hectares\n","\n","# Apply the function to each feature in the collection\n","area_mapped = gnut.map(calc_area)\n","\n","# Extract and print the area of each polygon\n","areas_info = area_mapped.getInfo()\n","\n","# Extracting areas into a list for plotting\n","areas = [feature['properties']['area_ha'] for feature in areas_info['features']]\n","\n","# Plotting the distribution of polygon areas\n","plt.figure(figsize=(10, 6))\n","plt.hist(areas, bins='auto', color='skyblue', alpha=0.8, rwidth=0.85)\n","plt.title('Distribution of Polygon Areas in Ha for Groundnut')\n","plt.xlabel('Area (Ha)')\n","plt.ylabel('Frequency')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"2uPvoOP8Uujo"},"source":["Show area of each crop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gvCoUdYvUPu_"},"outputs":[],"source":["#  'gnut', 'gsorrel', 'rice', etc. are already defined as ee.Geometry() or ee.Feature() objects representing each crop\n","crop_types = {\n","    'Gnut': gnut,\n","    'G. Sorrel': gsorrel,\n","    'Rice': rice,\n","    'Sesame': sesame,\n","    'Soye': soye,\n","    'Watermelon': watermelon,\n","    'Cassava': cassava,\n","    'Gnut Mix': gnutmix,\n","    'Millet Mix': milletmix,\n","    'Okra': okra,\n","    'Sorgh': sorgh,\n","    'Wheat': wheat,\n","    'Millet': millet,\n","    'Cowpea Mix': cowpmix,\n","    'Fallow': fallow\n","}\n","\n","# Calculate area for each crop and store in a dictionary\n","#areas = {name: crop.geometry().area().divide(10000).getInfo() for name, crop in crop_types.items()} #In Hectares\n","areas = {name: crop.geometry().area().getInfo() for name, crop in crop_types.items()}\n","\n","\n","# Create a DataFrame from the area dictionary\n","area_df = pd.DataFrame(list(areas.items()), columns=['Crop Type', 'Area (Squere meteres M2)'])\n","\n","# Display the DataFrame\n","print(area_df)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jF3CRVX0V1wl"},"source":["Graphic of each crop in area coverages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AbKXQ5YfVRtK"},"outputs":[],"source":["area_df = pd.DataFrame(list(areas.items()), columns=['Crop Type', 'Area (M2)'])\n","\n","# Sort the DataFrame by area for better visualization\n","area_df.sort_values('Area (M2)', ascending=False, inplace=True)\n","\n","# Plotting\n","plt.figure(figsize=(12, 8))\n","plt.bar(area_df['Crop Type'], area_df['Area (M2)'], color='skyblue')\n","plt.xlabel('Crop Type')\n","plt.ylabel('Area in M2')\n","plt.title('Area of Each Crop Type for FY2023')\n","plt.xticks(rotation=90)  # Rotate crop type names for better readability\n","plt.grid(False)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"lOcmfMnU0CuD"},"source":["### Count Pixels (NICFI) for each crop field represnetation of each specteral bands e.g Groundnut"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M2KnVvBp0XoH"},"outputs":[],"source":["# time range\n","start_date = '2023-09-01'\n","end_date = '2023-09-30'\n","\n","# Load the Planet/NICFI imagery\n","imagery = ee.ImageCollection('projects/planet-nicfi/assets/basemaps/africa').filterDate(start_date, end_date).filterBounds(merged2023)  # Assuming 'gnut' is the ee.Geometry() of my area of interest\n","#bands info\n","bands = ee.ImageCollection(imagery).first()\n","print(bands.bandNames().getInfo())\n","\n","#################  Planet Pixel count ####### Actual pixel count from PLANET\n","\n","# Function to mask and count pixels within the 'gnut' polygon\n","def count_pixels(image):\n","    # Mask the image with the polygon\n","    masked_image = image.clip(gnut) #remember 1 polygone selected for closer see demo\n","\n","    # Count pixels - 4.77 X 4.77 m (~5X5m) actual Planet/NICFI pixel resolution\n","    pixel_count = masked_image.reduceRegion(\n","        reducer=ee.Reducer.count(),\n","        geometry=gnut,\n","        scale=4.77,  # Scale in meters; 5m resolution take time to estimate (timeout)\n","        # to avoid time out estimation use 10X10m and convert it in to 5X5 by devide the result 4\n","        maxPixels=1e9  # Adjust maxPixels if needed to handle large areas\n","    )\n","\n","    # Need to return an ee.Feature for the .map() function\n","    return ee.Feature(None, pixel_count)\n","\n","# Apply the pixel counting to each image in the collection\n","pixel_counts = imagery.map(count_pixels)\n","\n","# Get information from the ImageCollection\n","pixel_counts_info = pixel_counts.getInfo()\n","\n","# Print out the results\n","for feature in pixel_counts_info['features']:\n","    print(feature['properties'])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TzGYAjPY9tf0"},"outputs":[],"source":["# Define the time range\n","start_date = '2023-09-01'\n","end_date = '2023-09-30'\n","\n","\n","# Load the Planet/NICFI imagery\n","imagery = ee.ImageCollection('projects/planet-nicfi/assets/basemaps/africa') \\\n","            .filterDate(start_date, end_date) \\\n","            .filterBounds(merged2023)\n","\n","# Print band information\n","bands = imagery.first()\n","print(bands.bandNames().getInfo())\n","\n","# Function to mask and count pixels within the AOI\n","def count_pixels(image):\n","    # Mask the image with the polygon\n","    masked_image = image.clip(merged2023)\n","\n","    # Count pixels - 10m resolution for faster computation\n","    pixel_count = masked_image.reduceRegion(\n","        reducer=ee.Reducer.count(),\n","        geometry=merged2023,\n","        scale=10,  # Scale in meters; coarser resolution for faster computation\n","        maxPixels=1e6  # Adjust maxPixels if needed to handle large areas\n","    )\n","\n","    # Need to return an ee.Feature for the .map() function\n","    return ee.Feature(None, pixel_count)\n","\n","# Apply the pixel counting to each image in the collection\n","pixel_counts = imagery.map(count_pixels)\n","\n","# Get information from the ImageCollection\n","pixel_counts_info = pixel_counts.getInfo()\n","\n","# Print out the results\n","for feature in pixel_counts_info['features']:\n","    print(feature['properties'])"]},{"cell_type":"markdown","metadata":{"id":"jKC_1ils9tNo"},"source":[]},{"cell_type":"markdown","metadata":{"id":"FJAcGlTUfoKE"},"source":["### pixel esimates for each crop considering area not pixels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XaJnKXD9Z6l7"},"outputs":[],"source":["# Assuming these are the individual crop polygons. others to be added from other crop years.\n","crop_types = {\n","    'gnut': gnut, 'gsorrel': gsorrel, 'rice': rice, 'sesame': sesame,\n","    'soye': soye, 'watermelon': watermelon, 'cassava': cassava,\n","    'gnutmix': gnutmix, 'milletmix': milletmix, 'okra': okra,\n","    'sorgh': sorgh, 'wheat': wheat, 'millet': millet, 'cowpmix': cowpmix,\n","    'fallow': fallow\n","}\n","\n","def count_pixels(crop_name, crop_polygon):\n","    # Filter the imagery to the bounds of the crop polygon\n","    crop_imagery = imagery.filterBounds(crop_polygon)\n","\n","    # Apply the pixel counting for each image in the filtered collection\n","    def pixel_count(image):\n","        masked_image = image.clip(crop_polygon)\n","        pixel_count = masked_image.reduceRegion(\n","            reducer=ee.Reducer.count(),\n","            geometry=crop_polygon,\n","            scale=10,  # Adjust the scale based on the actual resolution of NICFI imagery\n","            maxPixels=1e9\n","        )\n","        return ee.Feature(None, pixel_count)\n","\n","    pixel_counts = crop_imagery.map(pixel_count)\n","    pixel_counts_info = pixel_counts.getInfo()\n","\n","    # Print results for each crop type\n","    print(crop_name)\n","    for feature in pixel_counts_info['features']:\n","        print(feature['properties'])\n","\n","# Run the pixel counting for each crop type\n","for crop_name, crop_polygon in crop_types.items():\n","    count_pixels(crop_name, crop_polygon)"]},{"cell_type":"markdown","metadata":{"id":"qXZyvvqShrCc"},"source":["### Pixel Exclusion representes less (e.g 40%) with in inside the polygon area.\n","\n","Masking Pixels: Pixels with less than 40% coverage by the polygon are excluded using the mask method, where mask = fraction.gte(0.4) creates a mask that includes only pixels where the fraction is 40% or more."]},{"cell_type":"markdown","metadata":{"id":"PEJuJdTiiXr8"},"source":["- Band Selection: When creating the mask, the code now ensures that a single band is selected using image.select(0). This selects the first band of the image, assuming the first band is suitable for your masking purposes. Adjust the band selection if necessary based on the bands available in your specific imagery.\n","\n","- Mask Application: By using single_band_image.mask() in the fraction calculation, we ensure that any operations involving masks deal with a single-band image, thus avoiding band mismatch errors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Mj5TnDQhkEx"},"outputs":[],"source":["# Function to mask and count pixels within the 'gnut' polygon\n","def count_pixels(image):\n","    # Ensure that we work with a single band for mask creation\n","    # Here, you could potentially select a specific band or reduce to a single band\n","    # As an example, if the image has multiple bands, use just one (e.g., the first band) for mask calculations\n","    single_band_image = image.select(1)\n","\n","    # Calculate the fraction of the pixel area that overlaps with the polygon using a constant image\n","    constantImage = ee.Image.constant(1).clip(gnut)\n","    fraction = constantImage.updateMask(single_band_image.mask()).reduceRegion(\n","        reducer=ee.Reducer.mean(),\n","        geometry=gnut,\n","        scale=5,  # estimation with planet 5x5 is very difficult\n","        maxPixels=1e9\n","    ).get('constant')\n","\n","    # Threshold the fraction to include only pixels with >= 40% coverage\n","    mask = ee.Image(fraction).gte(0.4)\n","\n","    # Mask the image with the calculated mask\n","    masked_image = image.updateMask(mask)\n","\n","    # Count pixels\n","    pixel_count = masked_image.reduceRegion(\n","        reducer=ee.Reducer.count(),\n","        geometry=gnut,\n","        scale=5,\n","        maxPixels=1e9\n","    )\n","\n","    # Return an ee.Feature for the .map() function to work properly\n","    return ee.Feature(None, pixel_count)\n","\n","# Apply the pixel counting to each image in the collection\n","pixel_counts = imagery.map(count_pixels)\n","\n","# Get information from the ImageCollection\n","pixel_counts_info = pixel_counts.getInfo()\n","\n","# Print out the results\n","for feature in pixel_counts_info['features']:\n","    print(feature['properties'])"]},{"cell_type":"markdown","metadata":{"id":"9k0jFk_e9ChT"},"source":["only one polygon from gnut only for learning use"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJhG_Dmb8zna"},"outputs":[],"source":["# Function to mask and count pixels within the 'gnut' polygon\n","def count_pixels(image):\n","    # Ensure that we work with a single band for mask creation\n","    single_band_image = image.select(1)  # Select the first band (assuming 1-based index)\n","\n","    # Calculate the fraction of the pixel area that overlaps with the polygon using a constant image\n","    constant_image = ee.Image.constant(1).clip(polygon1)\n","    fraction = constant_image.updateMask(single_band_image.mask()).reduceRegion(\n","        reducer=ee.Reducer.mean(),\n","        geometry=gnut.geometry(),\n","        scale=5,  # estimation with planet 5x5 is very difficult\n","        maxPixels=1e9\n","    ).get('constant')\n","\n","    # Threshold the fraction to include only pixels with >= 40% coverage\n","    mask = ee.Image.constant(fraction).gte(0.4)\n","\n","    # Mask the image with the calculated mask\n","    masked_image = image.updateMask(mask)\n","\n","    # Count pixels\n","    pixel_count = masked_image.reduceRegion(\n","        reducer=ee.Reducer.count(),\n","        geometry=gnut.geometry(),\n","        scale=5,\n","        maxPixels=1e9\n","    )\n","\n","    # Return an ee.Feature for the .map() function to work properly\n","    return ee.Feature(None, pixel_count)\n","\n","# Apply the pixel counting to each image in the collection\n","pixel_counts = imagery.map(count_pixels)\n","\n","# Get information from the ImageCollection\n","pixel_counts_info = pixel_counts.getInfo()\n","\n","# Print out the results\n","for feature in pixel_counts_info['features']:\n","    print(feature['properties'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ILNKnZCXKUX"},"outputs":[],"source":["# Function to mask and count pixels within the 'gnut' polygon\n","def count_pixels(image):\n","    # Create a single band composite by averaging RGB and NIR bands\n","    composite = image.expression(\n","        \"(R + G + B + N) / 4\",  # Expression combining the bands\n","        {\n","            'R': image.select('R'),  #  'R' is the Red band\n","            'G': image.select('G'),  # 'G' is the Green band\n","            'B': image.select('B'),  #  'B' is the Blue band\n","            'N': image.select('N')   #  'N' is the NIR band\n","        }\n","    )\n","\n","    # Generate a mask based on the composite value, adjusting threshold as necessary\n","    mask = composite.gt(0.2)  # Example threshold\n","\n","    # Mask the image with the generated mask\n","    masked_image = image.updateMask(mask)\n","\n","    # Count pixels\n","    pixel_count = masked_image.reduceRegion(\n","        reducer=ee.Reducer.count(),\n","        geometry=gnut,\n","        scale=5,\n","        maxPixels=1e9\n","    )\n","\n","    # Return an ee.Feature for the .map() function to work properly\n","    return ee.Feature(None, pixel_count)\n","\n","# Apply the pixel counting to each image in the collection\n","pixel_counts = imagery.map(count_pixels)\n","\n","# Get information from the ImageCollection\n","pixel_counts_info = pixel_counts.getInfo()\n","\n","# Print out the results\n","for feature in pixel_counts_info['features']:\n","    print(feature['properties'])"]},{"cell_type":"markdown","metadata":{"id":"4A-gnC1OuL9G"},"source":["Pixel counts using area coverage regardless of the specteral bands (NICFI + 4.77m X 4.77 M resolutions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Bi2CGb9hUmU"},"outputs":[],"source":["resolution = 4.77  # Resolution of NICFI imagery in meters (4.77m x 4.77m per pixel)\n","\n","\n","# Function to calculate pixel count\n","def calculate_pixels(crop):\n","    area = crop.geometry().area()  # Area in square meters\n","    pixel_area = resolution * resolution  # Area of one pixel in square meters\n","    pixel_count = area.divide(pixel_area)  # Number of pixels\n","    return pixel_count.getInfo()\n","\n","# Calculate pixel count for each crop and store in a dictionary\n","pixel_counts = {name: calculate_pixels(crop) for name, crop in crop_types.items()}\n","\n","# Create a DataFrame from the area and pixel count dictionaries\n","area_df = pd.DataFrame(list(pixel_counts.items()), columns=['Crop Type', 'Pixel Count'])\n","\n","# Sort the DataFrame by 'Pixel Count' in descending order\n","area_df = area_df.sort_values(by='Pixel Count', ascending=False)\n","# Display the DataFrame\n","print(area_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jjx1cZ7akX9y"},"outputs":[],"source":["# Plotting\n","#plt.figure(figsize=(10, 6))\n","#plt.bar(area_df['Crop Type'], area_df['Pixel Count'], color='green')\n","#plt.xlabel('Crop Type')\n","#plt.ylabel('Pixel Count')\n","#plt.title('Pixel Count by Crop Type (Ascending Order)')\n","#plt.xticks(rotation=45, ha='right')\n","#plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels\n","#plt.show()\n","####################\n","# Plotting using seaborn lib.\n","plt.figure(figsize=(12, 8))\n","sns.barplot(x='Crop Type', y='Pixel Count', data=area_df, palette='viridis')  # Using 'viridis' palette for varying colors\n","plt.xlabel('Crop Type')\n","plt.ylabel('Pixel Count')\n","plt.title('Pixel Count by Crop Type - for data base 2(FY2023)')\n","plt.xticks(rotation=45, ha='right')\n","plt.tight_layout()  # Adjust layout to make room for the rotated x-axis labels\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"VQ6KJlrreX7K"},"source":["Area vs Pixel count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mE924kWEev5z"},"outputs":[],"source":["# Assuming 'crop_types' dictionary is predefined with ee.Geometry() objects\n","resolution = 4.77  # Resolution of NICFI imagery in meters\n","\n","# Function to calculate pixel count\n","def calculate_pixels(area):\n","    pixel_area = resolution * resolution  # Area of one pixel in square meters\n","    return area / pixel_area\n","\n","# Calculate area and pixel count for each crop and store in dictionaries\n","area_and_pixels = []\n","for name, crop in crop_types.items():\n","    area = crop.geometry().area().getInfo()  # Area in square meters\n","    pixel_count = calculate_pixels(area)\n","    area_and_pixels.append((name, area, pixel_count))\n","\n","# Create a DataFrame from the results\n","df = pd.DataFrame(area_and_pixels, columns=['Crop Type', 'Area (Square Meters)', 'Pixel Count'])\n","\n","# Sort the DataFrame by 'Pixel Count' in descending order (optional)\n","df.sort_values(by='Pixel Count', ascending=False, inplace=True)\n","\n","# Plotting\n","fig, ax1 = plt.subplots(figsize=(14, 8))\n","\n","# Bar plot for Area using Seaborn\n","color = 'tab:red'\n","sns.barplot(x='Crop Type', y='Area (Square Meters)', data=df, palette='viridis', ax=ax1)\n","ax1.set_xlabel('Crop Type')\n","ax1.set_ylabel('Area (Square Meters)', color=color)\n","ax1.tick_params(axis='y', labelcolor=color)\n","\n","# Create a twin Axes sharing the x-axis for Pixel Count\n","ax2 = ax1.twinx()\n","color = 'tab:blue'\n","sns.lineplot(x='Crop Type', y='Pixel Count', data=df, sort=False, marker='o', color='red', ax=ax2)\n","ax2.set_ylabel('Pixel Count', color=color)\n","ax2.tick_params(axis='y', labelcolor=color)\n","\n","# Improve layout and set x-axis labels rotation\n","plt.xticks(rotation=90)\n","fig.tight_layout()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"istZFBD0pNjF"},"source":["### Garba\n","\n","Hi Garba, please use the above functions and generate the mean/median pixel specteral band values for \"R\", \"G\", \"B\" and \"N\" values of each pixel for each crop. Let me know if you need my assitance or have questions on the above functions. Perhaps you can show it in vector/tabular form.\n","\n","--------------------------------\n","R      |    G  |     B  |     N\n","-------------------------------\n","       |       |        |  "]},{"cell_type":"markdown","metadata":{"id":"wp0LBrEmCF9E"},"source":["**Steps**"]},{"cell_type":"markdown","metadata":{"id":"qPf6cPF1Sak1"},"source":["To enhance the quality and usability of the images for further analysis or applications Normalization and calibration of images are essential processes in image processing and computer vision.  "]},{"cell_type":"markdown","metadata":{"id":"J9_EfsreQFHk"},"source":["**1. Data Collection**: Obtain multispectral images of the crops that include the required spectral bands (R, G, B, and NIR)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V9VAcmv-QGQy"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"vW0KWuhtQdRJ"},"source":["**2. Image Preprocessing**:"]},{"cell_type":"markdown","metadata":{"id":"BmSmJNVOSxOZ"},"source":["2.1. Image Normalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yypQbbs4S0pX"},"outputs":[],"source":["\n","\n","# Load the image\n","image = cv2.imread('/path/to/your/image.jpg')\n","\n","# Convert BGR image to RGB\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","# Normalize the image to range [0, 1]\n","normalized_image = image / 255.0\n","\n","# Alternatively, you can normalize to range [-1, 1]\n","# normalized_image = (image / 127.5) - 1.0\n","\n","# Display the original and normalized images\n","plt.subplot(1, 2, 1)\n","plt.title('Original Image')\n","plt.imshow(image)\n","\n","plt.subplot(1, 2, 2)\n","plt.title('Normalized Image')\n","plt.imshow(normalized_image)\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"6bEH3KzTQnKp"},"source":["2.2. Calibration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hxc9YkgeQtQS"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import glob\n","\n","# Termination criteria for corner sub-pixel accuracy\n","criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n","\n","# Prepare object points (0,0,0), (1,0,0), (2,0,0), ..., (6,5,0)\n","objp = np.zeros((6*7, 3), np.float32)\n","objp[:, :2] = np.mgrid[0:7, 0:6].T.reshape(-1, 2)\n","\n","# Arrays to store object points and image points from all images\n","objpoints = []  # 3d points in real world space\n","imgpoints = []  # 2d points in image plane\n","\n","# Load calibration images\n","images = glob.glob('/path/to/calibration/images/*.jpg')\n","\n","for fname in images:\n","    img = cv2.imread(fname)\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","    # Find the chessboard corners\n","    ret, corners = cv2.findChessboardCorners(gray, (7, 6), None)\n","\n","    # If found, add object points, image points (after refining them)\n","    if ret:\n","        objpoints.append(objp)\n","        corners2 = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)\n","        imgpoints.append(corners2)\n","\n","        # Draw and display the corners\n","        img = cv2.drawChessboardCorners(img, (7, 6), corners2, ret)\n","        cv2.imshow('img', img)\n","        cv2.waitKey(500)\n","\n","cv2.destroyAllWindows()\n","\n","# Perform camera calibration\n","ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n","\n","# Save the calibration results\n","np.savez('calibration_data.npz', mtx=mtx, dist=dist, rvecs=rvecs, tvecs=tvecs)\n","\n","# Undistort an image using the calibration results\n","img = cv2.imread('/path/to/your/test/image.jpg')\n","h, w = img.shape[:2]\n","newcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w, h), 1, (w, h))\n","\n","# Undistort\n","dst = cv2.undistort(img, mtx, dist, None, newcameramtx)\n","\n","# Crop the image\n","x, y, w, h = roi\n","dst = dst[y:y+h, x:x+w]\n","\n","# Display the result\n","cv2.imshow('calibrated', dst)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()\n"]},{"cell_type":"markdown","metadata":{"id":"dHp5FV9LQwZu"},"source":["2.3. Alignment"]},{"cell_type":"markdown","metadata":{"id":"VXSHt0wTOjKa"},"source":["  **  Install OpenCV**: Install OpenCV in your Colab environment to use its functionalities.\n","\n","    **Load and Display Images**: Load the two images you want to align. Display them using Matplotlib to ensure they are loaded correctly.\n","\n","    **Detect ORB Features and Compute Descriptors**: Use the ORB detector to find keypoints and compute descriptors for both images.\n","\n","    **Match Features Using BFMatcher**: Match the descriptors using BFMatcher, and sort the matches based on their distance (quality).\n","\n","    **Find Homography and Warp Image**: Extract the coordinates of the matched points, compute the homography matrix using RANSAC, and apply the homography to warp the second image to align it with the first image."]},{"cell_type":"markdown","metadata":{"id":"goAziACvPDk3"},"source":["1. **Install OpenCV**: First, ensure you have OpenCV installed in your Colab environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fHbd1sgXPHo7"},"outputs":[],"source":["!pip install opencv-python-headless\n"]},{"cell_type":"markdown","metadata":{"id":"oPIGL4ENPLSw"},"source":["**2. Load and Display Images**: Load the images you want to align."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YOmwztSFPTVD"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Load the images in grayscale\n","img1 = cv2.imread('/path/to/your/image1.jpg', cv2.IMREAD_GRAYSCALE)\n","img2 = cv2.imread('/path/to/your/image2.jpg', cv2.IMREAD_GRAYSCALE)\n","\n","# Display the images\n","plt.subplot(1, 2, 1)\n","plt.title('Image 1')\n","plt.imshow(img1, cmap='gray')\n","\n","plt.subplot(1, 2, 2)\n","plt.title('Image 2')\n","plt.imshow(img2, cmap='gray')\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"e322vR2IPWSJ"},"source":["**3. Detect ORB Features and Compute Descriptor**s:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FEY3EivzPcmu"},"outputs":[],"source":["# Initialize the ORB detector\n","orb = cv2.ORB_create()\n","\n","# Detect keypoints and compute descriptors\n","keypoints1, descriptors1 = orb.detectAndCompute(img1, None)\n","keypoints2, descriptors2 = orb.detectAndCompute(img2, None)\n"]},{"cell_type":"markdown","metadata":{"id":"MAFunTOGPf9o"},"source":["**4. Match Features Using BFMatcher**:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjYdzM1BPo8B"},"outputs":[],"source":["# Create BFMatcher object\n","bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n","\n","# Match descriptors\n","matches = bf.match(descriptors1, descriptors2)\n","\n","# Sort them in the order of their distance\n","matches = sorted(matches, key=lambda x: x.distance)\n","\n","# Draw top matches\n","img_matches = cv2.drawMatches(img1, keypoints1, img2, keypoints2, matches[:50], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n","\n","# Display the matches\n","plt.figure(figsize=(20, 10))\n","plt.imshow(img_matches)\n","plt.title('Feature Matches')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"g7L70895PwYz"},"source":["**5. Find Homography and Warp Image:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SnjJP9wvOwpS"},"outputs":[],"source":["# Extract location of good matches\n","points1 = np.zeros((len(matches), 2), dtype=np.float32)\n","points2 = np.zeros((len(matches), 2), dtype=np.float32)\n","\n","for i, match in enumerate(matches):\n","    points1[i, :] = keypoints1[match.queryIdx].pt\n","    points2[i, :] = keypoints2[match.trainIdx].pt\n","\n","# Find homography matrix\n","h, mask = cv2.findHomography(points2, points1, cv2.RANSAC)\n","\n","# Use homography to warp the image\n","height, width = img1.shape\n","aligned_img = cv2.warpPerspective(img2, h, (width, height))\n","\n","# Display the aligned image\n","plt.figure(figsize=(10, 10))\n","plt.subplot(1, 2, 1)\n","plt.title('Reference Image')\n","plt.imshow(img1, cmap='gray')\n","\n","plt.subplot(1, 2, 2)\n","plt.title('Aligned Image')\n","plt.imshow(aligned_img, cmap='gray')\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"dbVIGmM0MZ4a"},"source":["**Extract Pixel Values: Extract the pixel values for each spectral band (R, G, B, NIR) for each segmented crop area.**"]},{"cell_type":"markdown","metadata":{"id":"Y7pwVmGkQ2sU"},"source":["Clipping"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2dqHNv9zQ6kL"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"f9X91TDhQ63t"},"source":["3. **Segmentation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vcx5V2x7RFcE"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"j2b-hNkOR7H6"},"source":["**4. Extract Pixel Values**: Extract the pixel values for each spectral band (R, G, B, NIR) for each segmented crop area."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjFFEtWvR97w"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"MS2G36cPSCTg"},"source":[]},{"cell_type":"markdown","metadata":{"id":"JVd_6vKWMcwM"},"source":["**5. Compute Statistics:**\n","\n","    Mean: Calculate the mean value for each spectral band across all pixels for each crop.\n","    Median: Calculate the median value for each spectral band across all pixels for each crop."]},{"cell_type":"markdown","metadata":{"id":"paLLSEy7GqzS"},"source":["### Cyrille\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwxxInMsG5Aa"},"outputs":[],"source":[]}],"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1YtJE8vuA8cAX53zdjesD1-rqACmXm-Y3","timestamp":1717255012743}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}